{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01jy1uK6124k"
      },
      "source": [
        "# Импорт и установка всего\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5LPi-h-I0XS",
        "outputId": "204c9262-7cfe-485c-963b-649e593b58f7"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 43.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrVkXNYu1tDg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAVl72Mjdv8u",
        "outputId": "4138f448-71ab-4cfe-875a-ab7be6f021d7"
      },
      "source": [
        "!pip install summa"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 39.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 51 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=5a28439ec4beb8c6c62a8b84df0aff04df4e66170332763b5167003f7bb3a41b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/64/ac/7b443477588d365ef37ada30d456bdf5f07dc5be9f6324cb6e\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bsOCWZgLrUL",
        "outputId": "c34bb27b-40b4-438c-dead-d55ec6998fac"
      },
      "source": [
        "!pip install python-rake"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-rake\n",
            "  Downloading python_rake-1.5.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: python-rake\n",
            "Successfully installed python-rake-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_YYd1147256",
        "outputId": "e59a0d8c-a3b0-4cb5-c938-4fb781abd9d2"
      },
      "source": [
        "!pip install yargy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yargy\n",
            "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 30 kB 43.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 101 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (from yargy) (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->yargy) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->yargy) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->yargy) (2.4.417127.4579844)\n",
            "Installing collected packages: yargy\n",
            "Successfully installed yargy-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGP5SFiuLL93",
        "outputId": "2ae6c1c6-1e78-4683-ce54-3b31b910be42"
      },
      "source": [
        "import pymorphy2\n",
        "import nltk\n",
        "import numpy as np\n",
        "import RAKE\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from string import digits\n",
        "from yargy import Parser\n",
        "from yargy.pipelines import morph_pipeline\n",
        "from pymorphy2.tokenizers import simple_word_tokenize\n",
        "from yargy import rule, or_\n",
        "from yargy.interpretation import fact\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from summa import keywords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from yargy import Parser, rule, and_\n",
        "from yargy.predicates import gram, dictionary\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopword = stopwords.words('russian')\n",
        "stopword.append('это')\n",
        "stopword.append('который')\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "pymorph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAcJsGz818X4"
      },
      "source": [
        "# Основная часть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_K0Bw11_TR"
      },
      "source": [
        "Чтение датафрейма"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZolDTQ_CNv3K"
      },
      "source": [
        "df = pd.read_csv('corpus_tags.csv', delimiter=';')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkJe4Hvri7a-"
      },
      "source": [
        "Устройство датафрейма (csv-файл лежит в папке с тетрадкой):\n",
        "\n",
        "\n",
        "*   Text -- полный текст новости\n",
        "*   Source -- ссылка текст (все тексты взяты с РИА Новости)\n",
        "*   Original Tags -- ключевые слова с сайта\n",
        "*   My Tags -- мои ключевые слова\n",
        "*   Gold Tags -- эталон. За эталон я брал пересечение моих ключевых слов и оригинальных. Иногда в оригинальных ключевых словах можно встретить фамилию автора текста или жанр, например, \"Новости\" или \"В мире\". Такие ключевые слова я выкидывал из эталона\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "eSpmXGmwi57H",
        "outputId": "e0636cc8-4c6e-48c9-9926-6c2791fefaaf"
      },
      "source": [
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Source</th>\n",
              "      <th>Original Tags</th>\n",
              "      <th>My Tags</th>\n",
              "      <th>Gold Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>В Дании задержали российское судно В датском п...</td>\n",
              "      <td>https://ria.ru/20211104/daniya-1757704473.html</td>\n",
              "      <td>Дания, В мире, Россия</td>\n",
              "      <td>Дания, Россия, Академик Иоффе</td>\n",
              "      <td>Дания, Россия, Академик Иоффе</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ученые сообщили о крупнейшей за несколько лет ...</td>\n",
              "      <td>https://ria.ru/20211104/burya-1757695232.html</td>\n",
              "      <td>Наука, Физический Институт РАН, Российская ака...</td>\n",
              "      <td>Земля, Магнитные бури, Ученые, Магнитное поле</td>\n",
              "      <td>Земля, Магнитные бури, Ученые, Магнитное поле</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Одно страшнее другого. Установлена связь COVID...</td>\n",
              "      <td>https://ria.ru/20211102/kovid-1757233335.html</td>\n",
              "      <td>Наука, Здоровье, Биология, болезнь Альцгеймера...</td>\n",
              "      <td>Коронавирус, деменция, COVID-19, болезнь Альцг...</td>\n",
              "      <td>Коронавирус,COVID-19, болезнь Альцгеймера, исс...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Миллиардеры объединились против власти. Они не...</td>\n",
              "      <td>https://ria.ru/20211102/dengi-1757271767.html</td>\n",
              "      <td>США, Марк Цукерберг, Джо Байден, Франклин Рузв...</td>\n",
              "      <td>США, Марк Цукерберг, Джо Байден, Amazon, Apple...</td>\n",
              "      <td>США, Марк Цукерберг, Джо Байден, Amazon, Apple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ВОЗ спрогнозировала еще полмиллиона смертей от...</td>\n",
              "      <td>https://ria.ru/20211104/koronavirus-1757707409...</td>\n",
              "      <td>Распространение коронавируса, В мире, ВОЗ, Здо...</td>\n",
              "      <td>ВОЗ, COVID-19, Клюге</td>\n",
              "      <td>ВОЗ, COVID-19, Клюге</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EMA прокомментировало информацию о нарушениях ...</td>\n",
              "      <td>https://ria.ru/20211104/vaktsina-1757697051.html</td>\n",
              "      <td>Распросранение коронавируса, В Мире, Вакцина P...</td>\n",
              "      <td>Вакцина, EMA, BMJ, Pfizer, испытания</td>\n",
              "      <td>Вакцина, EMA, BMJ, Pfizer, испытания, США</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...                                          Gold Tags\n",
              "0  В Дании задержали российское судно В датском п...  ...                      Дания, Россия, Академик Иоффе\n",
              "1  Ученые сообщили о крупнейшей за несколько лет ...  ...      Земля, Магнитные бури, Ученые, Магнитное поле\n",
              "2  Одно страшнее другого. Установлена связь COVID...  ...  Коронавирус,COVID-19, болезнь Альцгеймера, исс...\n",
              "3  Миллиардеры объединились против власти. Они не...  ...  США, Марк Цукерберг, Джо Байден, Amazon, Apple...\n",
              "4  ВОЗ спрогнозировала еще полмиллиона смертей от...  ...                               ВОЗ, COVID-19, Клюге\n",
              "5  EMA прокомментировало информацию о нарушениях ...  ...          Вакцина, EMA, BMJ, Pfizer, испытания, США\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wz9IiSg2BIc"
      },
      "source": [
        "Функции для препроца текста и извлечения ключевых слов тремя способами, извлечения эталона из корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpyIHfY9B7oc"
      },
      "source": [
        "def preprocessing(text, tokenizer_method=simple_word_tokenize):\n",
        "    lemmas = []\n",
        "\n",
        "    for t in tokenizer_method(text):\n",
        "      lemma = pymorph.parse(t)[0].normal_form\n",
        "      lemmas.append(lemma)\n",
        "    \n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "def get_rake_tags(normalized_text, n):\n",
        "    tags = []\n",
        "    rake = RAKE.Rake(stopword)\n",
        "    tag_list = rake.run(normalized_text, maxWords=2, minFrequency=2)\n",
        "\n",
        "    for tag in tag_list:\n",
        "      if tag[1] > 0:\n",
        "        tags.append(tag[0])\n",
        "    \n",
        "    return tags[:n]\n",
        "\n",
        "\n",
        "def get_textrank_tags(normalized_text, n):\n",
        "  tag_list = keywords.keywords(normalized_text, language='russian', \n",
        "                           additional_stopwords=stopword)\n",
        "  return tag_list.split('\\n')[:n]\n",
        "\n",
        "\n",
        "### \"Это для tfidf тэггинга\" ###\n",
        "def sort_coo(coo_matrix):\n",
        "  tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "  return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "  sorted_items = sorted_items[:topn]\n",
        "  score_vals = []\n",
        "  feature_vals = []\n",
        "\n",
        "  for idx, score in sorted_items:\n",
        "      score_vals.append(round(score, 3))\n",
        "      feature_vals.append(feature_names[idx])\n",
        "\n",
        "  results= {}\n",
        "\n",
        "  for idx in range(len(feature_vals)):\n",
        "      results[feature_vals[idx]]=score_vals[idx]\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def get_tfidf_tags(vectorizer, feature_names, normalized_text, n):\n",
        "  tf_idf_vector = vectorizer.transform([normalized_text])\n",
        "  sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "  tags=extract_topn_from_vector(feature_names,sorted_items, n)\n",
        "  return list(tags.keys())\n",
        "### \"Конец\" ###\n",
        "\n",
        "\n",
        "def get_tags_docs(frame, tagging_method, n, column='Preprocessed'):\n",
        "  tags = []\n",
        "\n",
        "  if tagging_method == get_tfidf_tags:\n",
        "    vectorizer = TfidfVectorizer(stop_words=stopword, smooth_idf=True, \n",
        "                             use_idf=True, ngram_range=(1,3))\n",
        "    vectorizer.fit_transform(frame['Text'])\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "  for doc in frame[column]:\n",
        "    text = doc\n",
        "    if tagging_method != get_tfidf_tags:\n",
        "      tag_set = tagging_method(text, n)\n",
        "    else:\n",
        "      tag_set = tagging_method(vectorizer, feature_names, text, n)\n",
        "    \n",
        "    tags.append(tag_set)\n",
        "  \n",
        "  return tags\n",
        "\n",
        "\n",
        "def get_gold_tags(frame, column='Gold Tags'):\n",
        "  tags = []\n",
        "\n",
        "  for i in range(0, len(frame)):\n",
        "    tags_set = preprocessing(frame[column][i])\n",
        "    tags_set = [tag.strip() for tag in tags_set.split(',')]\n",
        "    tags_set = [tag.replace('covid-19', 'covid19') for tag in tags_set]\n",
        "    tags.append(tags_set)\n",
        " \n",
        "  return tags\n",
        "\n",
        "\n",
        "def get_metrics(predicted_tags, gold_tags):\n",
        "  preсisions = []\n",
        "  recalls = []\n",
        "  f_scores = []\n",
        "\n",
        "  for id, tag_set in enumerate(gold_tags):\n",
        "    doc = gold_tags[id]\n",
        "    TP = 0\n",
        "    \n",
        "    for i in range(0, len(predicted_tags[id])):\n",
        "      if predicted_tags[id][i] in doc:\n",
        "        TP+=1\n",
        "    \n",
        "    precision = TP/len(predicted_tags[id])\n",
        "    preсisions.append(precision)\n",
        "    recall = TP/len(doc)\n",
        "    recalls.append(recall)\n",
        "    if precision != 0 and recall != 0:\n",
        "      f_score = 2*precision*recall/(precision+recall)\n",
        "    else:\n",
        "      f_score = 0\n",
        "    f_scores.append(f_score)\n",
        "  return preсisions, recalls, f_scores"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdgvBvAI2fmF"
      },
      "source": [
        "Препроц каждого текста в корпусе. Замена covid-19 на covid19, чтобы TfIdf-векторайзер не считал это за разные слова (он так делает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZdaqYn_ei5a"
      },
      "source": [
        "df['Preprocessed'] = df.apply(lambda row: preprocessing(row['Text']), axis=1)\n",
        "df['Preprocessed'] = df.apply(lambda row: \n",
        "                              row['Preprocessed'].replace('covid-19', 'covid19'), \n",
        "                              axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNSxhD6jN-tV"
      },
      "source": [
        "gold_tags = get_gold_tags(df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_2xnLxlApZ2",
        "outputId": "46882769-62da-4080-dffd-f4bbb135b5a2"
      },
      "source": [
        "gold_tags"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['дания', 'россия', 'академик иофф'],\n",
              " ['земля', 'магнитный буря', 'учёный', 'магнитный поле'],\n",
              " ['коронавирус',\n",
              "  'covid19',\n",
              "  'болезнь альцгеймер',\n",
              "  'исследование',\n",
              "  'учёный',\n",
              "  'деменция'],\n",
              " ['сша',\n",
              "  'марк цукерберг',\n",
              "  'джо байден',\n",
              "  'amazon',\n",
              "  'apple',\n",
              "  'microsoft',\n",
              "  'дональд трамп',\n",
              "  'капитолий',\n",
              "  'илона маск',\n",
              "  'налог',\n",
              "  'закон'],\n",
              " ['воз', 'covid19', 'клюг'],\n",
              " ['вакцина', 'ema', 'bmj', 'pfizer', 'испытание', 'сша']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5VTOqkaQF4"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=stopword, smooth_idf=True, \n",
        "                             use_idf=True, ngram_range=(1,2), min_df=0.2,\n",
        "                             max_df=0.8)\n",
        "vectorizer.fit_transform(df['Text'])\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VeKW5BiOsI3"
      },
      "source": [
        "rake_tags = get_tags_docs(df, get_rake_tags, 15)\n",
        "textrank_tags = get_tags_docs(df, get_textrank_tags, 15)\n",
        "tfidf_tags = get_tags_docs(df, get_tfidf_tags, 15)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcfNboECXGqx",
        "outputId": "00280960-043c-4eb3-d89d-9cbc3714a009"
      },
      "source": [
        "rake_tags"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['академик иофф'],\n",
              " ['сохраняться ещё', 'земля', 'фиан', 'крупный'],\n",
              " ['контрольный группа',\n",
              "  'болезнь альцгеймер',\n",
              "  'концентрация внимание',\n",
              "  'учёный',\n",
              "  'человек',\n",
              "  'память',\n",
              "  'мышление',\n",
              "  'covid19',\n",
              "  'исследование',\n",
              "  'причём',\n",
              "  'ивл'],\n",
              " ['список forbes',\n",
              "  'дональд трамп',\n",
              "  'налог',\n",
              "  'трамп',\n",
              "  'инвестиция',\n",
              "  'пока',\n",
              "  'доход',\n",
              "  'взять',\n",
              "  'будущее',\n",
              "  'миллиардер',\n",
              "  'продать',\n",
              "  'прибыль',\n",
              "  'марс',\n",
              "  'twitter',\n",
              "  'попасть'],\n",
              " ['covid19', 'европа', 'человек'],\n",
              " ['сомнение вывод', 'безопасность', 'нарушение', 'ставить', 'эффективность']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25v96mjReJpF",
        "outputId": "7030fc80-4693-4832-82cd-a87111917089"
      },
      "source": [
        "textrank_tags"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['судный',\n",
              "  'задержать российский судно',\n",
              "  'академик иофф',\n",
              "  'научно',\n",
              "  'научный',\n",
              "  'посольство',\n",
              "  'обеспечительный',\n",
              "  'распространение',\n",
              "  'третий сторона',\n",
              "  'инцидент произойти',\n",
              "  'датский порт скаген',\n",
              "  'советский'],\n",
              " ['год магнитная буре',\n",
              "  'магнитный буря',\n",
              "  'солнечный',\n",
              "  'учёный',\n",
              "  'вспышка',\n",
              "  'земля',\n",
              "  'около час',\n",
              "  'сияние',\n",
              "  'зона',\n",
              "  'последний',\n",
              "  'северный',\n",
              "  'сутки',\n",
              "  'межпланетный',\n",
              "  'шкала',\n",
              "  'фиан'],\n",
              " ['covid',\n",
              "  'мозг',\n",
              "  'деменции',\n",
              "  'деменция',\n",
              "  'болезнь альцгеймер',\n",
              "  'тест',\n",
              "  'иммунный',\n",
              "  'связь',\n",
              "  'учёный',\n",
              "  'исследование',\n",
              "  'ген',\n",
              "  'поражать человек',\n",
              "  'клетка',\n",
              "  'больший',\n",
              "  'тяжёлый'],\n",
              " ['байден',\n",
              "  'налог',\n",
              "  'налоговый',\n",
              "  'весь',\n",
              "  'сенатор',\n",
              "  'доход',\n",
              "  'корпорация',\n",
              "  'законопроект мочь',\n",
              "  'демократ левый',\n",
              "  'компания',\n",
              "  'около',\n",
              "  'представитель',\n",
              "  'сми',\n",
              "  'большой миллиард',\n",
              "  'процент'],\n",
              " ['воз',\n",
              "  'полмиллиона',\n",
              "  'случай',\n",
              "  'время',\n",
              "  'миллион',\n",
              "  'европейский регион',\n",
              "  'мера',\n",
              "  'организация',\n",
              "  'год',\n",
              "  'ситуация',\n",
              "  'пять',\n",
              "  'пятый',\n",
              "  'спасти',\n",
              "  'вариант',\n",
              "  'индия'],\n",
              " ['ema',\n",
              "  'исследование',\n",
              "  'вакцина pfizer',\n",
              "  'сообщить',\n",
              "  'исследовательский организация сша',\n",
              "  'тысяча',\n",
              "  'серьёзность',\n",
              "  'серьёзный',\n",
              "  'европейский агентство',\n",
              "  'ventavia',\n",
              "  'около',\n",
              "  'comirnaty',\n",
              "  'основное',\n",
              "  'основный',\n",
              "  'риа']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVTS1XJx-dex",
        "outputId": "756856d6-1812-4959-b218-619e55a7d837"
      },
      "source": [
        "tfidf_tags"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['судно',\n",
              "  'академик',\n",
              "  'российский',\n",
              "  'судно академик',\n",
              "  'скаген',\n",
              "  'россия',\n",
              "  'посольство',\n",
              "  'научный',\n",
              "  'научно',\n",
              "  'использовать',\n",
              "  'институт',\n",
              "  'честь',\n",
              "  'физика',\n",
              "  'статус',\n",
              "  'сотрудник'],\n",
              " ['буря',\n",
              "  'уровень',\n",
              "  'магнитная',\n",
              "  'фиан',\n",
              "  'зона',\n",
              "  'буре',\n",
              "  'несколько',\n",
              "  'лишь',\n",
              "  'четыре',\n",
              "  'сохраняться',\n",
              "  'россия',\n",
              "  'прогноз',\n",
              "  'поле',\n",
              "  'нестабильность',\n",
              "  'мск'],\n",
              " ['болезнь',\n",
              "  'мозг',\n",
              "  'связь',\n",
              "  'человек',\n",
              "  'тест',\n",
              "  'коронавирус',\n",
              "  'когнитивный',\n",
              "  'симптом',\n",
              "  'исследование',\n",
              "  'иммунный',\n",
              "  'деменции',\n",
              "  'ген',\n",
              "  'несколько',\n",
              "  'риск',\n",
              "  'память'],\n",
              " ['налог',\n",
              "  'миллиард',\n",
              "  'байден',\n",
              "  'трамп',\n",
              "  'сенатор',\n",
              "  'платить',\n",
              "  'законопроект',\n",
              "  'доллар',\n",
              "  'страна',\n",
              "  'реформа',\n",
              "  'процент',\n",
              "  'президент',\n",
              "  'план',\n",
              "  'маск',\n",
              "  'компания'],\n",
              " ['полмиллиона',\n",
              "  'воз',\n",
              "  '2022',\n",
              "  'человек',\n",
              "  'страна',\n",
              "  'спасти',\n",
              "  'ситуация',\n",
              "  'данные',\n",
              "  'организация',\n",
              "  'время',\n",
              "  'человек полмиллиона',\n",
              "  'ханс',\n",
              "  'умереть',\n",
              "  'ужесточить',\n",
              "  'старший'],\n",
              " ['ema',\n",
              "  'исследование',\n",
              "  'испытание',\n",
              "  'безопасность',\n",
              "  'pfizer',\n",
              "  'сомнение',\n",
              "  'вывод',\n",
              "  'сша',\n",
              "  'пресс',\n",
              "  'информация',\n",
              "  'ventavia',\n",
              "  'pfizer biontech',\n",
              "  'comirnaty',\n",
              "  'bmj',\n",
              "  'biontech']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYEN-xXO3g9t"
      },
      "source": [
        "Посчитаем средние скоры для каждого метода извлечения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPbciAYYMrnQ"
      },
      "source": [
        "def get_mean_scores(tags, gold_tags):\n",
        "  tags_scores = get_metrics(tags, gold_tags)\n",
        "  tags_precision = np.mean(tags_scores[0])\n",
        "  tags_recall = np.mean(tags_scores[1])\n",
        "  tags_f_scores = np.mean(tags_scores[2])\n",
        "  \n",
        "  return 'precision: {}, recall: {}, f-score: {}'.format(\n",
        "      tags_precision, tags_recall, tags_f_scores)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "I-4GKotIdvDk",
        "outputId": "9a1a89c9-9a13-48bb-c9c0-fc0b95a24db0"
      },
      "source": [
        "get_mean_scores(rake_tags, gold_tags)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.34671717171717176, recall: 0.29419191919191917, f-score: 0.2846279537456008'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "whzM9rB0J5Zh",
        "outputId": "498e4c9a-fa51-4b4e-ee12-0ba92da309a7"
      },
      "source": [
        "get_mean_scores(textrank_tags, gold_tags)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.125, recall: 0.39015151515151514, f-score: 0.18555791187370133'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "qPPg2ObsebXy",
        "outputId": "9a538c9e-ff21-4c0f-ec45-e509f66d804e"
      },
      "source": [
        "get_mean_scores(tfidf_tags, gold_tags)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.1111111111111111, recall: 0.3207070707070707, f-score: 0.16096866096866094'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjel3jLF4DTH"
      },
      "source": [
        "В эталонах часто встречаются: сочетания существительное + существительное, прилагательное + существительное, просто существительное и имена. Еще в эталонных ключевых словах можно найти имена собственные на латиница: названия корпораций, вируса или вакцин, например, Pfizer и COVID-19. Напишем грамматику и извлечем только ключевые слова, которые удовлетворяют этим критериям."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpVAE95oni2r"
      },
      "source": [
        "# Функция для извлечения всех эталонных ключевых слов на латнице\n",
        "def extract_latin(gold_tag):\n",
        "  latin = []\n",
        "  for tag in gold_tags:\n",
        "    for t in tag:\n",
        "      if t.isascii():\n",
        "        latin.append(t)\n",
        "  return latin"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjLXCIQo9m7p"
      },
      "source": [
        "Name = fact(\n",
        "    'Name',\n",
        "    ['first', 'last']\n",
        ")\n",
        "\n",
        "\n",
        "NAME = rule(\n",
        "    gram('Name').interpretation(\n",
        "        Name.first.inflected()\n",
        "    ),\n",
        "    gram('Surn').interpretation(\n",
        "        Name.last.inflected()\n",
        "    )\n",
        ").interpretation(\n",
        "    Name\n",
        ")\n",
        "\n",
        "BRANDS = morph_pipeline(extract_latin(gold_tags))\n",
        "\n",
        "\n",
        "AdjP = rule(gram('ADJF'), gram('NOUN'))\n",
        "NP = rule(gram('NOUN'), gram('NOUN'))\n",
        "N = rule(gram('NOUN'))\n",
        "\n",
        "grammar = Parser(\n",
        "    or_(\n",
        "        NAME,\n",
        "        AdjP,\n",
        "        NP,\n",
        "        N,\n",
        "        BRANDS\n",
        "    )\n",
        ")"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0zCYGOk5MwO"
      },
      "source": [
        "Извлечем ключвые слова, которые соответствуют написанным нами правилам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwlkf6496Y8w"
      },
      "source": [
        "def template_tags(tag_set):\n",
        "  new_tags = []\n",
        "  for tag in tag_set:\n",
        "    for match in grammar.findall(tag):\n",
        "      new_tag = ' '.join([_.value for _ in match.tokens])\n",
        "      new_tag = new_tag.replace('covid 19', 'covid19')\n",
        "      new_tags.append(new_tag)\n",
        "  return new_tags"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7HBwJ8Y7jyt"
      },
      "source": [
        "def get_new_tag_sets(tags):\n",
        "  new_tag_sets = []\n",
        "  for tag in tags:\n",
        "    new_tag_sets.append(template_tags(tag))\n",
        "  return new_tag_sets"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj42ESIr5Se9"
      },
      "source": [
        "Посмотрим на метрики"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbkZE6wJDkeg"
      },
      "source": [
        "new_rake = get_new_tag_sets(rake_tags)\n",
        "new_textrank = get_new_tag_sets(textrank_tags)\n",
        "new_tfidf = get_new_tag_sets(tfidf_tags)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEvXHopQ--nu",
        "outputId": "b09f1b55-8d2d-4c7d-d3e1-fbe58af11323"
      },
      "source": [
        "new_rake"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['академик иофф'],\n",
              " ['земля', 'фиан'],\n",
              " ['контрольный группа',\n",
              "  'болезнь альцгеймер',\n",
              "  'концентрация внимание',\n",
              "  'учёный',\n",
              "  'человек',\n",
              "  'память',\n",
              "  'мышление',\n",
              "  'covid19',\n",
              "  'исследование'],\n",
              " ['список',\n",
              "  'дональд трамп',\n",
              "  'налог',\n",
              "  'трамп',\n",
              "  'инвестиция',\n",
              "  'доход',\n",
              "  'будущее',\n",
              "  'миллиардер',\n",
              "  'прибыль',\n",
              "  'марс'],\n",
              " ['covid19', 'европа', 'человек'],\n",
              " ['сомнение вывод', 'безопасность', 'нарушение', 'эффективность']]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "pIZWVXyaEg-O",
        "outputId": "7adfe344-97ee-4ed0-e783-8e6231d0f125"
      },
      "source": [
        "get_mean_scores(new_rake, gold_tags)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.412962962962963, recall: 0.29419191919191917, f-score: 0.31507936507936507'"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "xNJv7uH2Et-O",
        "outputId": "6dee1b68-8554-4973-bf8e-b2819c8096ff"
      },
      "source": [
        "get_mean_scores(new_textrank, gold_tags)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.21042152292152294, recall: 0.47348484848484845, f-score: 0.28577688651218064'"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "KyznN3qOE5Nc",
        "outputId": "316e3f29-8744-45de-b2d4-7c30286afc7d"
      },
      "source": [
        "get_mean_scores(new_tfidf, gold_tags)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.15331890331890333, recall: 0.34848484848484845, f-score: 0.2091005291005291'"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImN9xaL5qefl"
      },
      "source": [
        "Как можно видеть, правильно выделенные шаблоны увеличивают метрики. Главное, чтобы в правила и шаблоны попадали все ключевые слова из эталонов. Увеличение метрик (точности и f-score) происходит же в основном из-за сокращения числа предложенных ключевых слов, так как те слова, которые не подходят шаблонам не берутся в расчет.\n",
        "\n",
        "Кроме того, если применить фильтры, то из строк вроде \"дональд трамп продать\" будет выделяться то, что соответствует грамматике, например, имя \"дональд трамп\", что увеличивает количество правильно предсказанных ключевых слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOzJG9hbq4fK"
      },
      "source": [
        "# Что можно улучшить\n",
        "\n",
        "В-первую очередь, мне кажется разумным было бы как-то унифицировать имена. Например, можно выделить в качестве ключевого слова \"Джо Байден\", а извлекать программа будет ключевое слово \"Байден\". С одной стороны, это одно и то же, но для того, чтобы это отразилось на метриках, нужно чтобы слова полностью совпадали. Разумным при выделении имен было бы оставлять только фамилию, как в эталонных ключевых словах, так и в извлеченных.\n",
        "\n",
        "Еще можно было бы как-то конкретизировать существительные для шаблонов. Не просто выделять существительное, но на более большом корпусе текстов посмотреть на то какие существительные по-одиночку встречаются в эталонах. Например, одной из конкретизаций может быть выделение названий стран. Я не уверен, что это хорошая идея, потому что проще все таки сказать \"выделяй существительные по-одиночке\" чем перечислить какие они могут быть.\n",
        "\n",
        "В этой тетрадке я вручную ввожу максимульную длину ключевых слов (в данном случае 2). Можно было бы автоматически считать самое длинное ключевое слово в эталоне и предсказывать ключевые слова максимум его длины.\n",
        "\n",
        "P.S. Я также хотел бы немного высказаться по формулировке задания. Мне кажется важно сначала именно самому выделять в тексте ключевые слова, а потом смотреть как они в нем уже выделены. В противном случае как будто создается bias. Я, например, заметил, что выделяю почти те же ключевые слова и возможно, если бы я не видел оригинальные до этого, я бы сделал все чуть-чуть по-другому.\n",
        "\n"
      ]
    }
  ]
}
