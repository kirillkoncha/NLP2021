{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01jy1uK6124k"
      },
      "source": [
        "# Импорт и установка всего\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5LPi-h-I0XS",
        "outputId": "b598672d-d874-4389-aac0-e91732615516"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrVkXNYu1tDg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAVl72Mjdv8u",
        "outputId": "88fcc294-59c8-4cee-cfcd-69a626c379cb"
      },
      "source": [
        "!pip install summa"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: summa in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bsOCWZgLrUL",
        "outputId": "0f16bacc-2e64-4daa-800b-b5e68dfd800a"
      },
      "source": [
        "!pip install python-rake"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-rake in /usr/local/lib/python3.7/dist-packages (1.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_YYd1147256",
        "outputId": "8d0c1f27-93ca-4626-fb9b-27e553056772"
      },
      "source": [
        "!pip install yargy"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yargy in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (from yargy) (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->yargy) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->yargy) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->yargy) (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGP5SFiuLL93",
        "outputId": "3ced5fc7-6f8f-4b4b-dbfe-dce591557626"
      },
      "source": [
        "import pymorphy2\n",
        "import nltk\n",
        "import numpy as np\n",
        "import RAKE\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from string import digits\n",
        "from yargy import Parser\n",
        "from yargy.pipelines import morph_pipeline\n",
        "from pymorphy2.tokenizers import simple_word_tokenize\n",
        "from yargy import rule, or_\n",
        "from yargy.interpretation import fact\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from summa import keywords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from yargy import Parser, rule, and_\n",
        "from yargy.predicates import gram, dictionary\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopword = stopwords.words('russian')\n",
        "stopword.append('это')\n",
        "stopword.append('который')\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "pymorph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAcJsGz818X4"
      },
      "source": [
        "# Основная часть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_K0Bw11_TR"
      },
      "source": [
        "Чтение датафрейма"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZolDTQ_CNv3K"
      },
      "source": [
        "df = pd.read_csv('corpus_tags.csv', delimiter=';')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkJe4Hvri7a-"
      },
      "source": [
        "Устройство датафрейма (csv-файл лежит в папке с тетрадкой):\n",
        "\n",
        "\n",
        "*   Text -- полный текст новости\n",
        "*   Source -- ссылка текст (все тексты взяты с РИА Новости)\n",
        "*   Original Tags -- ключевые слова с сайта\n",
        "*   My Tags -- мои ключевые слова\n",
        "*   Gold Tags -- эталон. За эталон я брал пересечение моих ключевых слов и оригинальных. Иногда в оригинальных ключевых словах можно встретить фамилию автора текста или жанр, например, \"Новости\" или \"В мире\". Такие ключевые слова я выкидывал из эталона\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "eSpmXGmwi57H",
        "outputId": "6a56f347-702d-467d-fc02-164d1fd4af9a"
      },
      "source": [
        "df"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Source</th>\n",
              "      <th>Original Tags</th>\n",
              "      <th>My Tags</th>\n",
              "      <th>Gold Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>В Дании задержали российское судно В датском п...</td>\n",
              "      <td>https://ria.ru/20211104/daniya-1757704473.html</td>\n",
              "      <td>Дания, В мире, Россия</td>\n",
              "      <td>Дания, Россия, Академик Иоффе</td>\n",
              "      <td>Дания, Россия, Академик Иоффе</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ученые сообщили о крупнейшей за несколько лет ...</td>\n",
              "      <td>https://ria.ru/20211104/burya-1757695232.html</td>\n",
              "      <td>Наука, Физический Институт РАН, Российская ака...</td>\n",
              "      <td>Земля, Магнитные бури, Ученые, Магнитное поле</td>\n",
              "      <td>Земля, Магнитные бури, Ученые, Магнитное поле</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Одно страшнее другого. Установлена связь COVID...</td>\n",
              "      <td>https://ria.ru/20211102/kovid-1757233335.html</td>\n",
              "      <td>Наука, Здоровье, Биология, болезнь Альцгеймера...</td>\n",
              "      <td>Коронавирус, деменция, COVID-19, болезнь Альцг...</td>\n",
              "      <td>Коронавирус,COVID-19, болезнь Альцгеймера, исс...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Миллиардеры объединились против власти. Они не...</td>\n",
              "      <td>https://ria.ru/20211102/dengi-1757271767.html</td>\n",
              "      <td>США, Марк Цукерберг, Джо Байден, Франклин Рузв...</td>\n",
              "      <td>США, Марк Цукерберг, Джо Байден, Amazon, Apple...</td>\n",
              "      <td>США, Марк Цукерберг, Джо Байден, Amazon, Apple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ВОЗ спрогнозировала еще полмиллиона смертей от...</td>\n",
              "      <td>https://ria.ru/20211104/koronavirus-1757707409...</td>\n",
              "      <td>Распространение коронавируса, В мире, ВОЗ, Здо...</td>\n",
              "      <td>ВОЗ, COVID-19, Клюге</td>\n",
              "      <td>ВОЗ, COVID-19, Клюге</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EMA прокомментировало информацию о нарушениях ...</td>\n",
              "      <td>https://ria.ru/20211104/vaktsina-1757697051.html</td>\n",
              "      <td>Распросранение коронавируса, В Мире, Вакцина P...</td>\n",
              "      <td>Вакцина, EMA, BMJ, Pfizer, испытания</td>\n",
              "      <td>Вакцина, EMA, BMJ, Pfizer, испытания, США</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...                                          Gold Tags\n",
              "0  В Дании задержали российское судно В датском п...  ...                      Дания, Россия, Академик Иоффе\n",
              "1  Ученые сообщили о крупнейшей за несколько лет ...  ...      Земля, Магнитные бури, Ученые, Магнитное поле\n",
              "2  Одно страшнее другого. Установлена связь COVID...  ...  Коронавирус,COVID-19, болезнь Альцгеймера, исс...\n",
              "3  Миллиардеры объединились против власти. Они не...  ...  США, Марк Цукерберг, Джо Байден, Amazon, Apple...\n",
              "4  ВОЗ спрогнозировала еще полмиллиона смертей от...  ...                               ВОЗ, COVID-19, Клюге\n",
              "5  EMA прокомментировало информацию о нарушениях ...  ...          Вакцина, EMA, BMJ, Pfizer, испытания, США\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wz9IiSg2BIc"
      },
      "source": [
        "Функции для препроца текста и извлечения ключевых слов тремя способами, извлечения эталона из корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpyIHfY9B7oc"
      },
      "source": [
        "def preprocessing(text, tokenizer_method=simple_word_tokenize):\n",
        "    lemmas = []\n",
        "\n",
        "    for t in tokenizer_method(text):\n",
        "      lemma = pymorph.parse(t)[0].normal_form\n",
        "      lemmas.append(lemma)\n",
        "    \n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "def get_rake_tags(normalized_text, n):\n",
        "    tags = []\n",
        "    rake = RAKE.Rake(stopword)\n",
        "    tag_list = rake.run(normalized_text, maxWords=2, minFrequency=2)\n",
        "\n",
        "    for tag in tag_list:\n",
        "      if tag[1] > 0:\n",
        "        tags.append(tag[0])\n",
        "    \n",
        "    return tags[:n]\n",
        "\n",
        "\n",
        "def get_textrank_tags(normalized_text, n):\n",
        "  tag_list = keywords.keywords(normalized_text, language='russian', \n",
        "                           additional_stopwords=stopword)\n",
        "  return tag_list.split('\\n')[:n]\n",
        "\n",
        "\n",
        "### \"Это для tfidf тэггинга\" ###\n",
        "def sort_coo(coo_matrix):\n",
        "  tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "  return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "  sorted_items = sorted_items[:topn]\n",
        "  score_vals = []\n",
        "  feature_vals = []\n",
        "\n",
        "  for idx, score in sorted_items:\n",
        "      score_vals.append(round(score, 3))\n",
        "      feature_vals.append(feature_names[idx])\n",
        "\n",
        "  results= {}\n",
        "\n",
        "  for idx in range(len(feature_vals)):\n",
        "      results[feature_vals[idx]]=score_vals[idx]\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def get_tfidf_tags(vectorizer, feature_names, normalized_text, n):\n",
        "  tf_idf_vector = vectorizer.transform([normalized_text])\n",
        "  sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "  tags=extract_topn_from_vector(feature_names,sorted_items, n)\n",
        "  \n",
        "  return list(tags.keys())\n",
        "### \"Конец\" ###\n",
        "\n",
        "\n",
        "def get_tags_docs(frame, tagging_method, n, column='Preprocessed'):\n",
        "  tags = []\n",
        "\n",
        "  if tagging_method == get_tfidf_tags:\n",
        "    vectorizer = TfidfVectorizer(stop_words=stopword, smooth_idf=True, \n",
        "                             use_idf=True, ngram_range=(1,3))\n",
        "    vectorizer.fit_transform(frame['Text'])\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "  for doc in frame[column]:\n",
        "    text = doc\n",
        "    if tagging_method != get_tfidf_tags:\n",
        "      tag_set = tagging_method(text, n)\n",
        "    else:\n",
        "      tag_set = tagging_method(vectorizer, feature_names, text, n)\n",
        "    \n",
        "    tags.append(tag_set)\n",
        "  \n",
        "  return tags\n",
        "\n",
        "\n",
        "def get_gold_tags(frame, column='Gold Tags'):\n",
        "  tags = []\n",
        "\n",
        "  for i in range(0, len(frame)):\n",
        "    tags_set = preprocessing(frame[column][i])\n",
        "    tags_set = [tag.strip() for tag in tags_set.split(',')]\n",
        "    tags_set = [tag.replace('covid-19', 'covid19') for tag in tags_set]\n",
        "    tags.append(tags_set)\n",
        " \n",
        "  return tags\n",
        "\n",
        "\n",
        "def get_metrics(predicted_tags, gold_tags):\n",
        "  preсisions = []\n",
        "  recalls = []\n",
        "  f_scores = []\n",
        "\n",
        "  for id, tag_set in enumerate(gold_tags):\n",
        "    doc = gold_tags[id]\n",
        "    TP = 0\n",
        "    \n",
        "    for i in range(0, len(predicted_tags[id])):\n",
        "      if predicted_tags[id][i] in doc:\n",
        "        TP+=1\n",
        "    \n",
        "    precision = TP/len(predicted_tags[id])\n",
        "    preсisions.append(precision)\n",
        "    recall = TP/len(doc)\n",
        "    recalls.append(recall)\n",
        "    if precision != 0 and recall != 0:\n",
        "      f_score = 2*precision*recall/(precision+recall)\n",
        "    else:\n",
        "      f_score = 0\n",
        "    f_scores.append(f_score)\n",
        "  \n",
        "  return preсisions, recalls, f_scores"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdgvBvAI2fmF"
      },
      "source": [
        "Препроц каждого текста в корпусе. Замена covid-19 на covid19, чтобы TfIdf-векторайзер не считал это за разные слова (он так делает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZdaqYn_ei5a"
      },
      "source": [
        "df['Preprocessed'] = df.apply(lambda row: preprocessing(row['Text']), axis=1)\n",
        "df['Preprocessed'] = df.apply(lambda row: \n",
        "                              row['Preprocessed'].replace('covid-19', 'covid19'), \n",
        "                              axis=1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNSxhD6jN-tV"
      },
      "source": [
        "gold_tags = get_gold_tags(df)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_2xnLxlApZ2",
        "outputId": "b2d739d1-cdba-4ef3-d1f7-98fc317969b4"
      },
      "source": [
        "gold_tags"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['дания', 'россия', 'академик иофф'],\n",
              " ['земля', 'магнитный буря', 'учёный', 'магнитный поле'],\n",
              " ['коронавирус',\n",
              "  'covid19',\n",
              "  'болезнь альцгеймер',\n",
              "  'исследование',\n",
              "  'учёный',\n",
              "  'деменция'],\n",
              " ['сша',\n",
              "  'марк цукерберг',\n",
              "  'джо байден',\n",
              "  'amazon',\n",
              "  'apple',\n",
              "  'microsoft',\n",
              "  'дональд трамп',\n",
              "  'капитолий',\n",
              "  'илона маск',\n",
              "  'налог',\n",
              "  'закон'],\n",
              " ['воз', 'covid19', 'клюг'],\n",
              " ['вакцина', 'ema', 'bmj', 'pfizer', 'испытание', 'сша']]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5VTOqkaQF4"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=stopword, smooth_idf=True, \n",
        "                             use_idf=True, ngram_range=(1,2), min_df=0.2,\n",
        "                             max_df=0.8)\n",
        "vectorizer.fit_transform(df['Preprocessed'])\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VeKW5BiOsI3"
      },
      "source": [
        "rake_tags = get_tags_docs(df, get_rake_tags, 15)\n",
        "textrank_tags = get_tags_docs(df, get_textrank_tags, 15)\n",
        "tfidf_tags = get_tags_docs(df, get_tfidf_tags, 15)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcfNboECXGqx",
        "outputId": "ceab8912-49c5-40b6-c6de-db527dadce80"
      },
      "source": [
        "rake_tags"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['академик иофф'],\n",
              " ['сохраняться ещё', 'земля', 'фиан', 'крупный'],\n",
              " ['контрольный группа',\n",
              "  'болезнь альцгеймер',\n",
              "  'концентрация внимание',\n",
              "  'учёный',\n",
              "  'человек',\n",
              "  'память',\n",
              "  'мышление',\n",
              "  'covid19',\n",
              "  'исследование',\n",
              "  'причём',\n",
              "  'ивл'],\n",
              " ['список forbes',\n",
              "  'дональд трамп',\n",
              "  'налог',\n",
              "  'трамп',\n",
              "  'инвестиция',\n",
              "  'пока',\n",
              "  'доход',\n",
              "  'взять',\n",
              "  'будущее',\n",
              "  'миллиардер',\n",
              "  'продать',\n",
              "  'прибыль',\n",
              "  'марс',\n",
              "  'twitter',\n",
              "  'попасть'],\n",
              " ['covid19', 'европа', 'человек'],\n",
              " ['сомнение вывод', 'безопасность', 'нарушение', 'ставить', 'эффективность']]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25v96mjReJpF",
        "outputId": "0217544e-856d-4101-994a-e543eb0e0ae2"
      },
      "source": [
        "textrank_tags"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['судный',\n",
              "  'задержать российский судно',\n",
              "  'академик иофф',\n",
              "  'научно',\n",
              "  'научный',\n",
              "  'посольство',\n",
              "  'обеспечительный',\n",
              "  'распространение',\n",
              "  'третий сторона',\n",
              "  'инцидент произойти',\n",
              "  'датский порт скаген',\n",
              "  'советский'],\n",
              " ['год магнитная буре',\n",
              "  'магнитный буря',\n",
              "  'солнечный',\n",
              "  'учёный',\n",
              "  'вспышка',\n",
              "  'земля',\n",
              "  'около час',\n",
              "  'сияние',\n",
              "  'зона',\n",
              "  'последний',\n",
              "  'северный',\n",
              "  'сутки',\n",
              "  'межпланетный',\n",
              "  'шкала',\n",
              "  'фиан'],\n",
              " ['covid',\n",
              "  'мозг',\n",
              "  'деменции',\n",
              "  'деменция',\n",
              "  'болезнь альцгеймер',\n",
              "  'тест',\n",
              "  'иммунный',\n",
              "  'связь',\n",
              "  'учёный',\n",
              "  'исследование',\n",
              "  'ген',\n",
              "  'поражать человек',\n",
              "  'клетка',\n",
              "  'больший',\n",
              "  'тяжёлый'],\n",
              " ['байден',\n",
              "  'налог',\n",
              "  'налоговый',\n",
              "  'весь',\n",
              "  'сенатор',\n",
              "  'доход',\n",
              "  'корпорация',\n",
              "  'законопроект мочь',\n",
              "  'демократ левый',\n",
              "  'компания',\n",
              "  'около',\n",
              "  'представитель',\n",
              "  'сми',\n",
              "  'большой миллиард',\n",
              "  'процент'],\n",
              " ['воз',\n",
              "  'полмиллиона',\n",
              "  'случай',\n",
              "  'время',\n",
              "  'миллион',\n",
              "  'европейский регион',\n",
              "  'мера',\n",
              "  'организация',\n",
              "  'год',\n",
              "  'ситуация',\n",
              "  'пять',\n",
              "  'пятый',\n",
              "  'спасти',\n",
              "  'вариант',\n",
              "  'индия'],\n",
              " ['ema',\n",
              "  'исследование',\n",
              "  'вакцина pfizer',\n",
              "  'сообщить',\n",
              "  'исследовательский организация сша',\n",
              "  'тысяча',\n",
              "  'серьёзность',\n",
              "  'серьёзный',\n",
              "  'европейский агентство',\n",
              "  'ventavia',\n",
              "  'около',\n",
              "  'comirnaty',\n",
              "  'основное',\n",
              "  'основный',\n",
              "  'риа']]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVTS1XJx-dex",
        "outputId": "c9ff0c0a-aa53-48d1-98dd-de3ea553193e"
      },
      "source": [
        "tfidf_tags"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['судно',\n",
              "  'академик',\n",
              "  'российский',\n",
              "  'судно академик',\n",
              "  'скаген',\n",
              "  'россия',\n",
              "  'посольство',\n",
              "  'научный',\n",
              "  'научно',\n",
              "  'использовать',\n",
              "  'институт',\n",
              "  'честь',\n",
              "  'физика',\n",
              "  'статус',\n",
              "  'сотрудник'],\n",
              " ['буря',\n",
              "  'уровень',\n",
              "  'магнитная',\n",
              "  'фиан',\n",
              "  'зона',\n",
              "  'буре',\n",
              "  'несколько',\n",
              "  'лишь',\n",
              "  'четыре',\n",
              "  'сохраняться',\n",
              "  'россия',\n",
              "  'прогноз',\n",
              "  'поле',\n",
              "  'нестабильность',\n",
              "  'мск'],\n",
              " ['болезнь',\n",
              "  'мозг',\n",
              "  'связь',\n",
              "  'человек',\n",
              "  'тест',\n",
              "  'коронавирус',\n",
              "  'когнитивный',\n",
              "  'симптом',\n",
              "  'исследование',\n",
              "  'иммунный',\n",
              "  'деменции',\n",
              "  'ген',\n",
              "  'несколько',\n",
              "  'риск',\n",
              "  'память'],\n",
              " ['налог',\n",
              "  'миллиард',\n",
              "  'байден',\n",
              "  'трамп',\n",
              "  'сенатор',\n",
              "  'платить',\n",
              "  'законопроект',\n",
              "  'доллар',\n",
              "  'страна',\n",
              "  'реформа',\n",
              "  'процент',\n",
              "  'президент',\n",
              "  'план',\n",
              "  'маск',\n",
              "  'компания'],\n",
              " ['полмиллиона',\n",
              "  'воз',\n",
              "  '2022',\n",
              "  'человек',\n",
              "  'страна',\n",
              "  'спасти',\n",
              "  'ситуация',\n",
              "  'данные',\n",
              "  'организация',\n",
              "  'время',\n",
              "  'человек полмиллиона',\n",
              "  'ханс',\n",
              "  'умереть',\n",
              "  'ужесточить',\n",
              "  'старший'],\n",
              " ['ema',\n",
              "  'исследование',\n",
              "  'испытание',\n",
              "  'безопасность',\n",
              "  'pfizer',\n",
              "  'сомнение',\n",
              "  'вывод',\n",
              "  'сша',\n",
              "  'пресс',\n",
              "  'информация',\n",
              "  'ventavia',\n",
              "  'pfizer biontech',\n",
              "  'comirnaty',\n",
              "  'bmj',\n",
              "  'biontech']]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYEN-xXO3g9t"
      },
      "source": [
        "Посчитаем средние скоры для каждого метода извлечения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPbciAYYMrnQ"
      },
      "source": [
        "def get_mean_scores(tags, gold_tags):\n",
        "  tags_scores = get_metrics(tags, gold_tags)\n",
        "  tags_precision = np.mean(tags_scores[0])\n",
        "  tags_recall = np.mean(tags_scores[1])\n",
        "  tags_f_scores = np.mean(tags_scores[2])\n",
        "  \n",
        "  return 'precision: {}, recall: {}, f-score: {}'.format(\n",
        "      tags_precision, tags_recall, tags_f_scores)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "I-4GKotIdvDk",
        "outputId": "63477992-9db1-4f78-ffea-790a30fbbfc5"
      },
      "source": [
        "get_mean_scores(rake_tags, gold_tags)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.34671717171717176, recall: 0.29419191919191917, f-score: 0.2846279537456008'"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "whzM9rB0J5Zh",
        "outputId": "0b7be9cb-2df6-47c6-97c2-9da442639337"
      },
      "source": [
        "get_mean_scores(textrank_tags, gold_tags)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.125, recall: 0.39015151515151514, f-score: 0.18555791187370133'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "qPPg2ObsebXy",
        "outputId": "ef6cc6e7-419c-4348-e4c0-798aa380593f"
      },
      "source": [
        "get_mean_scores(tfidf_tags, gold_tags)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.1111111111111111, recall: 0.3207070707070707, f-score: 0.16096866096866094'"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjel3jLF4DTH"
      },
      "source": [
        "В эталонах часто встречаются: сочетания существительное + существительное, прилагательное + существительное, просто существительное и имена. Еще в эталонных ключевых словах можно найти имена собственные на латиница: названия корпораций, вируса или вакцин, например, Pfizer и COVID-19. Напишем грамматику и извлечем только ключевые слова, которые удовлетворяют этим критериям."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjLXCIQo9m7p"
      },
      "source": [
        "Name = fact(\n",
        "    'Name',\n",
        "    ['first', 'last']\n",
        ")\n",
        "\n",
        "\n",
        "NAME = rule(\n",
        "    gram('Name').interpretation(\n",
        "        Name.first.inflected()\n",
        "    ),\n",
        "    gram('Surn').interpretation(\n",
        "        Name.last.inflected()\n",
        "    )\n",
        ").interpretation(\n",
        "    Name\n",
        ")\n",
        "\n",
        "\n",
        "AdjP = rule(gram('ADJF'), gram('NOUN'))\n",
        "NP = rule(gram('NOUN'), gram('NOUN'))\n",
        "N = rule(gram('NOUN'))\n",
        "\n",
        "grammar = Parser(\n",
        "    or_(\n",
        "        NAME,\n",
        "        AdjP,\n",
        "        NP,\n",
        "        N\n",
        "    )\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0zCYGOk5MwO"
      },
      "source": [
        "Извлечем ключвые слова, которые соответствуют написанным нами правилам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwlkf6496Y8w"
      },
      "source": [
        "def template_tags(tag_set):\n",
        "  new_tags = []\n",
        "  \n",
        "  for tag in tag_set:\n",
        "    if tag.isascii():\n",
        "      new_tags.append(tag)\n",
        "    else:\n",
        "      for match in grammar.findall(tag):\n",
        "        new_tag = ' '.join([_.value for _ in match.tokens])\n",
        "        new_tag = new_tag.replace('covid 19', 'covid19')\n",
        "        new_tags.append(new_tag)\n",
        "  \n",
        "  return new_tags"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7HBwJ8Y7jyt"
      },
      "source": [
        "def get_new_tag_sets(tags):\n",
        "  new_tag_sets = []\n",
        "\n",
        "  for tag in tags:\n",
        "      new_tag_sets.append(template_tags(tag))\n",
        "  \n",
        "  return new_tag_sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj42ESIr5Se9"
      },
      "source": [
        "Посмотрим на метрики"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbkZE6wJDkeg"
      },
      "source": [
        "new_rake = get_new_tag_sets(rake_tags)\n",
        "new_textrank = get_new_tag_sets(textrank_tags)\n",
        "new_tfidf = get_new_tag_sets(tfidf_tags)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "pIZWVXyaEg-O",
        "outputId": "a478f6aa-fefb-4251-c5c0-bf76a9aed61b"
      },
      "source": [
        "get_mean_scores(new_rake, gold_tags)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.40993265993266, recall: 0.29419191919191917, f-score: 0.31363636363636366'"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "xNJv7uH2Et-O",
        "outputId": "ab093aaa-22e9-4c88-9c3c-b8c0f0f6e7b3"
      },
      "source": [
        "get_mean_scores(new_textrank, gold_tags)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.18643162393162394, recall: 0.4457070707070707, f-score: 0.258543771043771'"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "KyznN3qOE5Nc",
        "outputId": "4abb54b6-9bbf-4669-fbda-f262e5f15b01"
      },
      "source": [
        "get_mean_scores(new_tfidf, gold_tags)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'precision: 0.12427849927849927, recall: 0.3207070707070707, f-score: 0.1757671957671958'"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImN9xaL5qefl"
      },
      "source": [
        "Как можно видеть, правильно выделенные шаблоны увеличивают метрики. Главное, чтобы в правила и шаблоны попадали все ключевые слова из эталонов. Увеличение метрик (точности и f-score) происходит же в основном из-за сокращения числа предложенных ключевых слов, так как те слова, которые не подходят шаблонам не берутся в расчет.\n",
        "\n",
        "Кроме того, если применить фильтры, то из строк вроде \"дональд трамп продать\" будет выделяться то, что соответствует грамматике, например, имя \"дональд трамп\", что увеличивает количество правильно предсказанных ключевых слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOzJG9hbq4fK"
      },
      "source": [
        "# Что можно улучшить\n",
        "\n",
        "В-первую очередь, мне кажется разумным было бы как-то унифицировать имена. Например, можно выделить в качестве ключевого слова \"Джо Байден\", а извлекать программа будет ключевое слово \"Байден\". С одной стороны, это одно и то же, но для того, чтобы это отразилось на метриках, нужно чтобы слова полностью совпадали. Разумным при выделении имен было бы оставлять только фамилию, как в эталонных ключевых словах, так и в извлеченных.\n",
        "\n",
        "Еще можно было бы как-то конкретизировать существительные для шаблонов. Не просто выделять существительное, но на более большом корпусе текстов посмотреть на то какие существительные по-одиночку встречаются в эталонах. Например, одной из конкретизаций может быть выделение названий стран. И еще можно выделять, например, еще имена/названия компаний и продуктов (как на русском, так и на латинице), потому что они достаточно часто могут встречаться в текстахЯ не уверен, что это хорошая идея, потому что проще все таки сказать \"выделяй существительные по-одиночке\" чем перечислить какие они могут быть.\n",
        "\n",
        "В этой тетрадке я вручную ввожу максимульную длину ключевых слов (в данном случае 2). Можно было бы автоматически считать самое длинное ключевое слово в эталоне и предсказывать ключевые слова максимум его длины.\n",
        "\n",
        "P.S. Я также хотел бы немного высказаться по формулировке задания. Мне кажется важно сначала именно самому выделять в тексте ключевые слова, а потом смотреть как они в нем уже выделены. В противном случае как будто создается bias. Я, например, заметил, что выделяю почти те же ключевые слова и возможно, если бы я не видел оригинальные до этого, я бы сделал все чуть-чуть по-другому.\n",
        "\n"
      ]
    }
  ]
}